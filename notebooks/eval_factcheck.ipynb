{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a4bd19",
   "metadata": {},
   "source": [
    "# # ファクトチェック機能 性能評価ノートブック\n",
    "# \n",
    "# このノートブックでは、`services/factcheck.py`に実装された`factcheck_slide`関数の性能を評価します。\n",
    "# - **評価指標**: 適合率(Precision), 再現率(Recall), F1スコア\n",
    "# - **評価データ**: `data/ground_truth/*.json` に格納された正解データ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75c457",
   "metadata": {},
   "source": [
    "# ### 1. 必要なライブラリのインポートと設定\n",
    "# 必要なライブラリのインポート\n",
    "# バックエンドのソースコードをPythonパスに追加\n",
    "# 評価に必要な定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ce4f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# backendのソースコードをインポートするためにパスを追加\n",
    "sys.path.append(str(Path.cwd().parent / 'backend' / 'extraction_service'))\n",
    "\n",
    "from app.services.factcheck import factcheck_slide\n",
    "from app.services.vector_store import VectorStore # 全チャンク取得のために追加\n",
    "from app.models import Inconsistency\n",
    "\n",
    "# --- 定数設定 ---\n",
    "GROUND_TRUTH_DIR = Path.cwd().parent / 'data' / 'ground_truth'\n",
    "# 評価対象のスライドIDリスト（実際のファイル名に合わせてください）\n",
    "SLIDE_IDS_TO_EVALUATE = [\"slide-001\", \"slide-002\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054984ad",
   "metadata": {},
   "source": [
    "# ### 2. 評価データの読み込みと前処理\n",
    "# 正解データ（ground truth）をJSONファイルから読み込む\n",
    "# \"矛盾\"とラベル付けされたchunk_idのセットを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacc558",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_ground_truth(slide_id: str) -> set[str]:\n",
    "    \"\"\"正解データから「矛盾」とラベル付けされたchunk_idのセットを読み込む\"\"\"\n",
    "    gt_path = GROUND_TRUTH_DIR / f\"{slide_id}.json\"\n",
    "    if not gt_path.exists():\n",
    "        return set()\n",
    "    \n",
    "    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        # \"verdict\"が\"contradiction\"である項目のchunk_idをセットとして返す\n",
    "        return {item['chunk_id'] for item in data if item.get('verdict') == 'contradiction'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111dbf13",
   "metadata": {},
   "source": [
    "# ### 3. 評価の実行\n",
    "# 各スライドに対して`factcheck_slide`関数を実行し、予測結果と正解データを比較します。\n",
    "# 各スライドに対して：\n",
    "# 全チャンクIDを取得\n",
    "# 正解データを読み込み\n",
    "# ファクトチェック機能で予測を実行\n",
    "# 正解と予測のラベルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dac603",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "y_true = [] # 正解ラベル (1: 矛盾あり, 0: 矛盾なし)\n",
    "y_pred = [] # 予測ラベル (1: 矛盾あり, 0: 矛盾なし)\n",
    "\n",
    "for slide_id in SLIDE_IDS_TO_EVALUATE:\n",
    "    print(f\"--- Processing slide: {slide_id} ---\")\n",
    "    \n",
    "    # 1. スライド内の全チャンクIDを取得\n",
    "    all_chunks = vector_store.get_chunks_by_document_id(slide_id)\n",
    "    all_chunk_ids = {chunk.chunk_id for chunk in all_chunks}\n",
    "    if not all_chunk_ids:\n",
    "        print(f\"No chunks found for {slide_id}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 2. 正解データを読み込み\n",
    "    true_contradiction_chunks = load_ground_truth(slide_id)\n",
    "    print(f\"Ground Truth contradictions: {len(true_contradiction_chunks)} chunks\")\n",
    "\n",
    "    # 3. `factcheck_slide`関数で予測を実行\n",
    "    predicted_inconsistencies = factcheck_slide(slide_id)\n",
    "    predicted_contradiction_chunks = {\n",
    "        item.chunk_id for item in predicted_inconsistencies if item.verdict == 'contradiction'\n",
    "    }\n",
    "    print(f\"Predicted contradictions: {len(predicted_contradiction_chunks)} chunks\")\n",
    "\n",
    "    # 4. 全チャンクIDをループし、正解と予測のラベルを作成\n",
    "    for chunk_id in all_chunk_ids:\n",
    "        y_true.append(1 if chunk_id in true_contradiction_chunks else 0)\n",
    "        y_pred.append(1 if chunk_id in predicted_contradiction_chunks else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15755f9d",
   "metadata": {},
   "source": [
    "# ### 4. 評価指標の計算と表示\n",
    "# 以下の指標を計算：\n",
    "# Precision（適合率）\n",
    "# Recall（再現率）\n",
    "# F1スコア\n",
    "# 結果をPandas DataFrameで表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4b089",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not y_true:\n",
    "    print(\"Evaluation could not be performed. No data available.\")\n",
    "else:\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, \n",
    "        y_pred, \n",
    "        average='binary', # 「矛盾あり(1)」クラスに対する指標を計算\n",
    "        pos_label=1,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Overall Performance Metrics ---\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    print(\"---------------------------------\")\n",
    "    \n",
    "    # Pandas DataFrameで見やすく表示\n",
    "    results_df = pd.DataFrame({\n",
    "        'Metric': ['Precision', 'Recall', 'F1-Score'],\n",
    "        'Score': [precision, recall, f1]\n",
    "    })\n",
    "    display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ea774",
   "metadata": {},
   "source": [
    "### 5. 結果の可視化 (混同行列)\n",
    "# 混同行列（Confusion Matrix）を生成\n",
    "# matplotlib/seabornを使用して視覚化\n",
    "# 矛盾の検出精度を分かりやすく表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26986b01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if y_true:\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    disp = ConfusionMatrixDisplay(\n",
    "        confusion_matrix=cm, \n",
    "        display_labels=['Contradiction', 'Not Contradiction']\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "    ax.set_title(\"Fact-Check Confusion Matrix\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
